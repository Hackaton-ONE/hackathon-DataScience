# Hackathon-DataScience

# Sentiment Analysis API (High-Performance MVP)

> Uma API robusta, vetorizada e ass√≠ncrona para an√°lise de sentimentos em PT/ES, capaz de processar **+14.000 requisi√ß√µes/segundo**.

---

## √çndice:

- [Vis√£o Geral](#vis√£o-geral)
  - [Principais Diferenciais](#principais-diferenciais)
- [Benchmark de Performance](#benchmark-de-performance)
- [Stack Tecnol√≥gico](#stack-tecnol√≥gico)
- [Instala√ß√£o e Execu√ß√£o](#instala√ß√£o-e-execu√ß√£o)
- [Endpoints e Exemplos](#endpoints-e-exemplos)
- [Decis√µes de Arquitetura](#decis√µes-de-arquitetura)

---

## üî≠ Vis√£o Geral

Esta aplica√ß√£o foi desenhada para resolver o gargalo comum em deploy de modelos de ML: **lat√™ncia e escalabilidade**.

Diferente de abordagens tradicionais (loops linha-a-linha), esta API utiliza **processamento vetorial em lote**, **streaming de dados** e **cache em mem√≥ria**, permitindo a an√°lise de arquivos gigantescos com consumo m√≠nimo de mem√≥ria RAM e lat√™ncia baix√≠ssima.

## ‚ú® Principais Diferenciais

* **Dual Language:** Suporte nativo a Portugu√™s (PT) e Espanhol (ES).
* **True Streaming:** Processa arquivos CSV muito maiores que a mem√≥ria RAM dispon√≠vel (leitura em *chunks*).
* **High Performance:** Vetoriza√ß√£o com Scikit-Learn e NumPy.
* **Smart Caching:** Cache LRU para predi√ß√µes repetidas (lat√™ncia zero para textos frequentes).
* **Robustez:** Parser de CSV "blindado" contra erros de codifica√ß√£o e colunas duplicadas.
* **Compress√£o:** GZip middleware para otimiza√ß√£o de banda de rede.

---

## ‚ö° Benchmark de Performance

Testes de carga realizados em ambiente local (CPU padr√£o):

| Carga (Linhas) | Tamanho (CSV) | Tempo Total | Throughput (Req/s) |
| :--- | :--- | :--- | :--- |
| **10.000** | ~2.8 MB | 2.7 segundos | **~3.700** |
| **50.000** | ~14 MB | 3.6 segundos | **~13.800** |
| **100.000** | ~28 MB | 6.9 segundos | **~14.500** |

> *Nota: A arquitetura mant√©m o consumo de mem√≥ria constante, independentemente do tamanho do arquivo de entrada.*

---

## üõ†Ô∏è Stack Tecnol√≥gico

* **Python 3.11+**
* **FastAPI** (High-performance web framework)
* **Scikit-Learn** (Infer√™ncia Vetorizada)
* **Pandas** (Manipula√ß√£o eficiente de dados)
* **Joblib** (Serializa√ß√£o de Modelos)
* **Uvicorn** (Servidor ASGI)

---

## ‚ñ∂Ô∏è Instala√ß√£o e Execu√ß√£o

### Setup

```bash
# 1. Clonar o reposit√≥rio
git clone [https://github.com/Hackaton-ONE/hackathon-DataScience.git](https://github.com/Hackaton-ONE/hackathon-DataScience.git)
cd hackathon-DataScience/api

# 2. Criar ambiente virtual
python -m venv venv

# 3. Ativar (Windows)
venv\Scripts\activate
# Ativar (Linux/Mac)
source venv/bin/activate

# 4. Instalar depend√™ncias otimizadas
pip install -r requirements.txt
```
## Executar a API Localmente
```
uvicorn main:app --reload
```
Acesse a documenta√ß√£o interativa (Swagger UI): [http://127.0.0.1:8000](http://localhost:8000/docs)

---

## Endpoint

> `POST /sentiment/analyze`

Ponto √∫nico de entrada que aceita tanto JSON quanto Arquivos CSV.

O idioma padr√£o √© `pt`, mas pode ser alterado via query param `?lang=es`.


### A. JSON (Texto √önico/Single - Ideal para Chatbots)

**Requisi√ß√£o:**

```JSON
{
  "text": "O atendimento foi excelente e a entrega r√°pida!",
  "lang": "pt"
}
```

**Resposta:**

```JSON
{
  "idioma": "pt",
  "previsao": "Positivo",
  "probabilidade": 0.9850
}
```

### B. JSON (Batch - Lista de Textos)

**Requisi√ß√£o:**

```JSON
{
  "texts": ["Adorei o produto", "Demorou muito", "Qualidade m√©dia"]
}
```

**Resposta:**

```JSON
[
  {
    "idioma":"pt",
    "previsao":"Positivo",
    "probabilidade":0.9120
  },
    {
      "idioma":"pt",
      "previsao":"Negativo",
      "probabilidade":0.8530
    },
    {
      "idioma":"pt",
      "previsao":"Negativo",
      "probabilidade":0.6015
    }
]
}
```

---

### C. Arquivo CSV (Big Data / BI)

Ideal para processar hist√≥ricos de atendimento.

O arquivo √© processado via streaming e o download inicia imediatamente.

## Exemplo via cURL:

**Requisi√ß√£o:**

```Bash
curl -X POST "http://localhost:8000/sentiment/analyze?lang=pt" \
  -F "file=@meu_dataset_gigante.csv" \
  -o resultado_analise.csv
```

**Formato do CSV de Sa√≠da:** A API limpa automaticamente colunas duplicadas e retorna um CSV enxuto:

**Resposta:**

```Snippet de c√≥digo
text,idioma,previsao,probabilidade
"Adorei o produto",pt,Positivo,0.9874
"P√©ssimo servi√ßo",pt,Negativo,0.9289
```

---

## Decis√µes de Arquitetura

1. - **Vetoriza√ß√£o vs Loops:** Utilizamos `model.predict_proba(lista_inteira)` ao inv√©s de iterar linha por linha.
  Isso delega o c√°lculo matem√°tico para as bibliotecas em C (NumPy/BLAS), acelerando o processo em at√© 100x.

2. - **Streaming & Generators:** Para CSVs, utilizamos Python Generators (`yield`).
  A API l√™ blocos de 5.000 linhas, processa, devolve e limpa da mem√≥ria. Isso impede erros de Out of Memory (OOM).
  
3. - **IO Bound Optimization:** O uso de arquivos tempor√°rios e leitura otimizada (`chunksize`) garante que a CPU nunca fique ociosa esperando leitura de disco.

4. - **Smart Column Detection:**
     Para garantir robustez total, eliminamos a depend√™ncia de nomes de cabe√ßalhos (que podem estar errados ou em outros idiomas).
     O sistema realiza uma **an√°lise estat√≠stica do conte√∫do** em tempo real: calcula a m√©dia de caracteres das colunas n√£o-num√©ricas do bloco.
     Se a m√©dia for > 20 caracteres, a coluna √© automaticamente identificada como o texto a ser analisado.
     Isso torna a API imune a CSVs mal formatados ou com cabe√ßalhos enganosos.
